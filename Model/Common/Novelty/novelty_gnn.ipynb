{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5836bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78289d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joblib contains texts: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def embed_texts(texts, max_features=512, normalize=True):\n",
    "    \"\"\"\n",
    "    Simple text embedding using TF-IDF vectors.\n",
    "    Returns a list of numpy arrays (one per text). Vectors are optionally L2-normalized.\n",
    "    \"\"\"\n",
    "np.source\n",
    "# Create or update a joblib pre-trained embeddings file from DB (supabase)\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# attempt SBERT first, fallback to TF-IDF\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "except Exception:\n",
    "    sbert = None\n",
    "\n",
    "# supabase config from env (ensure these are set in your notebook env)\n",
    "SUPABASE_URL = os.getenv('SUPABASE_URL')\n",
    "SUPABASE_KEY = os.getenv('SUPABASE_KEY')\n",
    "try:\n",
    "    from supabase import create_client\n",
    "    sb = create_client(SUPABASE_URL, SUPABASE_KEY) if SUPABASE_URL and SUPABASE_KEY else None\n",
    "except Exception:\n",
    "    sb = None\n",
    "\n",
    "# fetch past novelty entries from DB table 'novelty_reports' (fallback to storage not implemented here)\n",
    "past_texts = []\n",
    "if sb is not None:\n",
    "    try:\n",
    "        res = sb.table('novelty_reports').select('filename,result').limit(2000).execute()\n",
    "        rows = getattr(res, 'data', []) or []\n",
    "        for r in rows:\n",
    "            content = r.get('result') or {}\n",
    "            # pick unique_sections if present, else raw snippet\n",
    "            for s in (content.get('unique_sections') or [])[:5]:\n",
    "                if s and s not in past_texts:\n",
    "                    past_texts.append(s)\n",
    "            raw = (content.get('raw_text') or content.get('raw') or '')\n",
    "            if raw and raw not in past_texts:\n",
    "                past_texts.append(raw[:1000])\n",
    "    except Exception as e:\n",
    "        print('Supabase read failed in notebook:', e)\n",
    "\n",
    "# prepare pretrain directory relative to this notebook path\n",
    "PRETRAIN_DIR = os.path.join(os.getcwd(), 'pre-trained')\n",
    "os.makedirs(PRETRAIN_DIR, exist_ok=True)\n",
    "JOBLIB_PATH = os.path.join(PRETRAIN_DIR, 'novelty_embeddings.joblib')\n",
    "\n",
    "def compute_and_save_embeddings(texts):\n",
    "    if not texts:\n",
    "        joblib.dump({'texts': [], 'embs': None}, JOBLIB_PATH)\n",
    "        return\n",
    "    if sbert is not None:\n",
    "        try:\n",
    "            embs = sbert.encode(texts, convert_to_tensor=False, show_progress_bar=True)\n",
    "            embs = np.vstack([np.array(e).astype(np.float32) for e in embs])\n",
    "            joblib.dump({'texts': texts, 'embs': embs.tolist()}, JOBLIB_PATH)\n",
    "            print('Saved SBERT embeddings to', JOBLIB_PATH)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print('SBERT embedding failed:', e)\n",
    "    # fallback to TF-IDF if SBERT not available\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vec = TfidfVectorizer(max_features=1024, stop_words='english')\n",
    "        X = vec.fit_transform(texts).toarray()\n",
    "        # normalize rows\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        norms[norms==0] = 1\n",
    "        X = (X / norms).astype(np.float32)\n",
    "        joblib.dump({'texts': texts, 'embs': X.tolist()}, JOBLIB_PATH)\n",
    "        print('Saved TF-IDF embeddings to', JOBLIB_PATH)\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print('TF-IDF fallback failed:', e)\n",
    "\n",
    "# deduplicate and limit size to keep file manageable\n",
    "unique_texts = []\n",
    "seen = set()\n",
    "for t in past_texts:\n",
    "    k = (t or '').strip()\n",
    "    if not k:\n",
    "        continue\n",
    "    if k in seen:\n",
    "        continue\n",
    "    seen.add(k)\n",
    "    unique_texts.append(k)\n",
    "    if len(unique_texts) >= 2000:\n",
    "        break\n",
    "\n",
    "compute_and_save_embeddings(unique_texts)\n",
    "\n",
    "# load back for verification\n",
    "try:\n",
    "    data = joblib.load(JOBLIB_PATH)\n",
    "    print('joblib contains texts:', len(data.get('texts', [])))\n",
    "except Exception as e:\n",
    "    print('Failed to load saved joblib:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a3f695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No labeled rows found to evaluate.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Supabase client (reuse env vars)\n",
    "SUPABASE_URL = os.getenv('SUPABASE_URL')\n",
    "SUPABASE_KEY = os.getenv('SUPABASE_KEY')\n",
    "try:\n",
    "    from supabase import create_client\n",
    "    sb = create_client(SUPABASE_URL, SUPABASE_KEY) if SUPABASE_URL and SUPABASE_KEY else None\n",
    "except Exception:\n",
    "    sb = None\n",
    "\n",
    "# Fetch labeled novelty rows\n",
    "rows = []\n",
    "if sb is not None:\n",
    "    try:\n",
    "        res = sb.table('novelty_reports').select('filename,novelty_percentage,result').limit(2000).execute()\n",
    "        rows = getattr(res, 'data', []) or []\n",
    "    except Exception as e:\n",
    "        print('Failed to read novelty_reports from Supabase:', e)\n",
    "\n",
    "# Build dataframe of text (representative) and label\n",
    "texts = []\n",
    "labels = []\n",
    "for r in rows:\n",
    "    res_json = (r.get('result') or {})\n",
    "    label = res_json.get('novelty_percentage') if res_json else r.get('novelty_percentage')\n",
    "    try:\n",
    "        label = int(label) if label is not None else None\n",
    "    except Exception:\n",
    "        label = None\n",
    "    # pick representative text: first unique_section or raw snippet\n",
    "    text = ''\n",
    "    if res_json:\n",
    "        us = res_json.get('unique_sections') or []\n",
    "        if us:\n",
    "            text = us[0]\n",
    "        else:\n",
    "            text = (res_json.get('raw_text') or res_json.get('raw') or '')[:1000]\n",
    "    if not text or label is None:\n",
    "        continue\n",
    "    texts.append(text)\n",
    "    labels.append(label)\n",
    "\n",
    "n = min(len(texts), 1000)\n",
    "if n == 0:\n",
    "    print('No labeled rows found to evaluate.')\n",
    "else:\n",
    "    texts = texts[:n]\n",
    "    labels = labels[:n]\n",
    "\n",
    "    # Try loading precomputed joblib embeddings (if available)\n",
    "    JOBLIB_PATH = os.path.join(os.getcwd(), 'pre-trained', 'novelty_embeddings.joblib')\n",
    "    emb_store = None\n",
    "    try:\n",
    "        data = joblib.load(JOBLIB_PATH)\n",
    "        emb_store = data.get('embs', None)\n",
    "        if emb_store is not None:\n",
    "            emb_store = np.asarray(emb_store)\n",
    "    except Exception:\n",
    "        emb_store = None\n",
    "\n",
    "    # Create embeddings for our evaluation texts (prefer SBERT, fallback TF-IDF)\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        eval_embs = embedder.encode(texts, convert_to_tensor=False, show_progress_bar=False)\n",
    "        eval_embs = np.vstack([np.array(e).astype(np.float32) for e in eval_embs])\n",
    "    except Exception as e:\n",
    "        print('SBERT not available; falling back to TF-IDF for evaluation:', e)\n",
    "        vec = TfidfVectorizer(max_features=1024, stop_words='english')\n",
    "        X = vec.fit_transform(texts).toarray()\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        norms[norms==0] = 1\n",
    "        eval_embs = (X / norms).astype(np.float32)\n",
    "\n",
    "    # Heuristic prediction: novelty = (1 - mean(top-k similarity to other docs)) * 100\n",
    "    preds = []\n",
    "    for i in range(len(eval_embs)):\n",
    "        past = np.delete(eval_embs, i, axis=0)\n",
    "        if past.shape[0] == 0:\n",
    "            preds.append(100.0)\n",
    "            continue\n",
    "        sims = cosine_similarity(eval_embs[i:i+1], past)[0]\n",
    "        k = min(5, len(sims))\n",
    "        topk = -np.sort(-sims)[:k]\n",
    "        mean_sim = float(np.mean(topk)) if len(topk) > 0 else 0.0\n",
    "        preds.append((1.0 - mean_sim) * 100.0)\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    mse = mean_squared_error(labels, preds)\n",
    "    rmse = mse ** 0.5\n",
    "    r2 = r2_score(labels, preds)\n",
    "    print(f'Count: {len(labels)}; MAE: {mae:.2f}; RMSE: {rmse:.2f}; R2: {r2:.3f}')\n",
    "\n",
    "    df = pd.DataFrame({'actual': labels, 'pred': preds})\n",
    "\n",
    "    # Scatter: actual vs predicted\n",
    "    plt.figure(figsize=(7,6))\n",
    "    sns.scatterplot(data=df, x='actual', y='pred', alpha=0.6)\n",
    "    plt.plot([0,100],[0,100], 'r--')\n",
    "    plt.xlabel('Actual Novelty (%)')\n",
    "    plt.ylabel('Predicted Novelty (%)')\n",
    "    plt.title('Actual vs Predicted Novelty')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Residuals histogram\n",
    "    df['error'] = df['pred'] - df['actual']\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.histplot(df['error'], bins=40, kde=True)\n",
    "    plt.title('Prediction Error Distribution (pred - actual)')\n",
    "    plt.xlabel('Error')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calibration: bin by actual novelty and show mean predicted vs actual per-bin\n",
    "    df['bin'] = pd.cut(df['actual'], bins=10)\n",
    "    calib = df.groupby('bin').agg(mean_actual=('actual','mean'), mean_pred=('pred','mean'), count=('actual','count')).reset_index()\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.lineplot(data=calib, x='mean_actual', y='mean_pred', marker='o')\n",
    "    plt.plot([0,100],[0,100],'r--')\n",
    "    plt.xlabel('Mean Actual Novelty (bin)')\n",
    "    plt.ylabel('Mean Predicted Novelty')\n",
    "    plt.title('Calibration by Actual Novelty Bins')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d52ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca4d6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 candidate rows for labeling\n",
      "No candidates found; ensure SUPABASE env vars are set and table exists\n"
     ]
    }
   ],
   "source": [
    "# Export labeling samples for manual annotation\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from collections import defaultdict\n",
    "SUPABASE_URL = os.getenv('SUPABASE_URL')\n",
    "SUPABASE_KEY = os.getenv('SUPABASE_KEY')\n",
    "try:\n",
    "    from supabase import create_client\n",
    "    sb = create_client(SUPABASE_URL, SUPABASE_KEY) if SUPABASE_URL and SUPABASE_KEY else None\n",
    "except Exception:\n",
    "    sb = None\n",
    "rows = []\n",
    "if sb is not None:\n",
    "    try:\n",
    "        res = sb.table('novelty_reports').select('filename,novelty_percentage,result').limit(5000).execute()\n",
    "        rows = getattr(res, 'data', []) or []\n",
    "    except Exception as e:\n",
    "        print('Failed to read novelty_reports from Supabase:', e)\n",
    "# Build candidates list\n",
    "candidates = []\n",
    "for r in rows:\n",
    "    res_json = (r.get('result') or {})\n",
    "    label = res_json.get('novelty_percentage') if res_json else r.get('novelty_percentage')\n",
    "    try:\n",
    "        label = int(label) if label is not None else None\n",
    "    except Exception:\n",
    "        label = None\n",
    "    text = ''\n",
    "    if res_json:\n",
    "        us = res_json.get('unique_sections') or []\n",
    "        if us:\n",
    "            text = us[0]\n",
    "        else:\n",
    "            text = (res_json.get('raw_text') or res_json.get('raw') or '')[:1000]\n",
    "    if not text:\n",
    "        continue\n",
    "    candidates.append({'filename': r.get('filename'), 'text': text, 'existing_label': label})\n",
    "print(f'Found {len(candidates)} candidate rows for labeling')\n",
    "if len(candidates) == 0:\n",
    "    print('No candidates found; ensure SUPABASE env vars are set and table exists')\n",
    "else:\n",
    "    # Option: stratified sample across existing label bins to get diverse set\n",
    "    bins = defaultdict(list)\n",
    "    for c in candidates:\n",
    "        lab = c['existing_label']\n",
    "        if lab is None:\n",
    "            bins['none'].append(c)\n",
    "        else:\n",
    "            b = int(lab // 10) * 10\n",
    "            bins[b].append(c)\n",
    "    # desired sample size (change as needed)\n",
    "    sample_size = 500\n",
    "    sampled = []\n",
    "    # take proportional from each bin but at least 1 from each non-empty bin\n",
    "    keys = sorted(bins.keys())\n",
    "    per_bin = max(1, sample_size // max(1, len(keys)))\n",
    "    for k in keys:\n",
    "        group = bins[k]\n",
    "        if len(group) <= per_bin:\n",
    "            sampled.extend(group)\n",
    "        else:\n",
    "            sampled.extend(random.sample(group, per_bin))\n",
    "    # if undersampled, fill randomly\n",
    "    if len(sampled) < sample_size:\n",
    "        remaining = [c for c in candidates if c not in sampled]\n",
    "        add = min(len(remaining), sample_size - len(sampled))\n",
    "        if add > 0:\n",
    "            sampled.extend(random.sample(remaining, add))\n",
    "    # prepare output dir\n",
    "    out_dir = os.path.join(os.getcwd(), 'pre-trained')\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, 'novelty_labeling_samples.csv')\n",
    "    # write CSV with columns: filename, text_snippet, existing_label, new_label, notes\n",
    "    with open(out_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['filename','text_snippet','existing_label','new_label','notes'])\n",
    "        for r in sampled:\n",
    "            writer.writerow([r['filename'], r['text'].replace('\\n',' ').strip(), r['existing_label'] if r['existing_label'] is not None else '', '', ''])\n",
    "    print('Wrote labeling CSV to', out_path)\n",
    "    print('Open the CSV, add `new_label` values (0-100), then save. After labeling, run the evaluation cell to compute metrics against your new labels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34060df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
