{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5836bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def embed_texts(texts, max_features=512, normalize=True):\n",
    "    \"\"\"\n",
    "    Simple text embedding using TF-IDF vectors.\n",
    "    Returns a list of numpy arrays (one per text). Vectors are optionally L2-normalized.\n",
    "    \"\"\"\n",
    "\n",
    "# Create or update a joblib pre-trained embeddings file from DB (supabase)\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# attempt SBERT first, fallback to TF-IDF\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "except Exception:\n",
    "    sbert = None\n",
    "\n",
    "# supabase config from env (ensure these are set in your notebook env)\n",
    "SUPABASE_URL = os.getenv('SUPABASE_URL')\n",
    "SUPABASE_KEY = os.getenv('SUPABASE_KEY')\n",
    "try:\n",
    "    from supabase import create_client\n",
    "    sb = create_client(SUPABASE_URL, SUPABASE_KEY) if SUPABASE_URL and SUPABASE_KEY else None\n",
    "except Exception:\n",
    "    sb = None\n",
    "\n",
    "# fetch past novelty entries from DB table 'novelty_reports' (fallback to storage not implemented here)\n",
    "past_texts = []\n",
    "if sb is not None:\n",
    "    try:\n",
    "        res = sb.table('novelty_reports').select('filename,result').limit(2000).execute()\n",
    "        rows = getattr(res, 'data', []) or []\n",
    "        for r in rows:\n",
    "            content = r.get('result') or {}\n",
    "            # pick unique_sections if present, else raw snippet\n",
    "            for s in (content.get('unique_sections') or [])[:5]:\n",
    "                if s and s not in past_texts:\n",
    "                    past_texts.append(s)\n",
    "            raw = (content.get('raw_text') or content.get('raw') or '')\n",
    "            if raw and raw not in past_texts:\n",
    "                past_texts.append(raw[:1000])\n",
    "    except Exception as e:\n",
    "        print('Supabase read failed in notebook:', e)\n",
    "\n",
    "# prepare pretrain directory relative to this notebook path\n",
    "PRETRAIN_DIR = os.path.join(os.getcwd(), 'pre-trained')\n",
    "os.makedirs(PRETRAIN_DIR, exist_ok=True)\n",
    "JOBLIB_PATH = os.path.join(PRETRAIN_DIR, 'novelty_embeddings.joblib')\n",
    "\n",
    "def compute_and_save_embeddings(texts):\n",
    "    if not texts:\n",
    "        joblib.dump({'texts': [], 'embs': None}, JOBLIB_PATH)\n",
    "        return\n",
    "    # Prefer SBERT embeddings when available\n",
    "    if sbert is not None:\n",
    "        try:\n",
    "            embs = sbert.encode(texts, convert_to_tensor=False, show_progress_bar=True)\n",
    "            embs = np.vstack([np.array(e).astype(np.float32) for e in embs])\n",
    "            # try to fit a nearest-neighbors index for faster lookup\n",
    "            try:\n",
    "                from sklearn.neighbors import NearestNeighbors\n",
    "                if embs.shape[0] > 1:\n",
    "                    nn = NearestNeighbors(n_neighbors=min(10, embs.shape[0]-1), metric='cosine', algorithm='auto')\n",
    "                    nn.fit(embs)\n",
    "                else:\n",
    "                    nn = None\n",
    "            except Exception as _e:\n",
    "                print('Failed to build NN index (SBERT):', _e)\n",
    "                nn = None\n",
    "            joblib.dump({'texts': texts, 'embs': embs.tolist(), 'nn': nn, 'backend': 'sbert'}, JOBLIB_PATH)\n",
    "            print('Saved SBERT embeddings to', JOBLIB_PATH)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print('SBERT embedding failed:', e)\n",
    "    # fallback to TF-IDF if SBERT not available\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vec = TfidfVectorizer(max_features=1024, stop_words='english')\n",
    "        X = vec.fit_transform(texts).toarray()\n",
    "        # normalize rows\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        norms[norms==0] = 1\n",
    "        X = (X / norms).astype(np.float32)\n",
    "        try:\n",
    "            from sklearn.neighbors import NearestNeighbors\n",
    "            if X.shape[0] > 1:\n",
    "                nn = NearestNeighbors(n_neighbors=min(10, X.shape[0]-1), metric='cosine', algorithm='auto')\n",
    "                nn.fit(X)\n",
    "            else:\n",
    "                nn = None\n",
    "        except Exception as _e:\n",
    "            print('Failed to build NN index (TF-IDF):', _e)\n",
    "            nn = None\n",
    "        joblib.dump({'texts': texts, 'embs': X.tolist(), 'nn': nn, 'vec': vec, 'backend': 'tfidf'}, JOBLIB_PATH)\n",
    "        print('Saved TF-IDF embeddings to', JOBLIB_PATH)\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print('TF-IDF fallback failed:', e)\n",
    "\n",
    "# deduplicate and limit size to keep file manageable\n",
    "unique_texts = []\n",
    "seen = set()\n",
    "for t in past_texts:\n",
    "    k = (t or '').strip()\n",
    "    if not k:\n",
    "        continue\n",
    "    if k in seen:\n",
    "        continue\n",
    "    seen.add(k)\n",
    "    unique_texts.append(k)\n",
    "    if len(unique_texts) >= 2000:\n",
    "        break\n",
    "\n",
    "compute_and_save_embeddings(unique_texts)\n",
    "\n",
    "# load back for verification\n",
    "try:\n",
    "    data = joblib.load(JOBLIB_PATH)\n",
    "    print('joblib contains texts:', len(data.get('texts', [])))\n",
    "except Exception as e:\n",
    "    print('Failed to load saved joblib:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a3f695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No labeled rows found to evaluate.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Supabase client (reuse env vars)\n",
    "SUPABASE_URL = os.getenv('SUPABASE_URL')\n",
    "SUPABASE_KEY = os.getenv('SUPABASE_KEY')\n",
    "try:\n",
    "    from supabase import create_client\n",
    "    sb = create_client(SUPABASE_URL, SUPABASE_KEY) if SUPABASE_URL and SUPABASE_KEY else None\n",
    "except Exception:\n",
    "    sb = None\n",
    "\n",
    "# Fetch labeled novelty rows\n",
    "rows = []\n",
    "if sb is not None:\n",
    "    try:\n",
    "        res = sb.table('novelty_reports').select('filename,novelty_percentage,result').limit(2000).execute()\n",
    "        rows = getattr(res, 'data', []) or []\n",
    "    except Exception as e:\n",
    "        print('Failed to read novelty_reports from Supabase:', e)\n",
    "\n",
    "# Build dataframe of text (representative) and label\n",
    "texts = []\n",
    "labels = []\n",
    "for r in rows:\n",
    "    res_json = (r.get('result') or {})\n",
    "    label = res_json.get('novelty_percentage') if res_json else r.get('novelty_percentage')\n",
    "    try:\n",
    "        label = int(label) if label is not None else None\n",
    "    except Exception:\n",
    "        label = None\n",
    "    # pick representative text: first unique_section or raw snippet\n",
    "    text = ''\n",
    "    if res_json:\n",
    "        us = res_json.get('unique_sections') or []\n",
    "        if us:\n",
    "            text = us[0]\n",
    "        else:\n",
    "            text = (res_json.get('raw_text') or res_json.get('raw') or '')[:1000]\n",
    "    if not text or label is None:\n",
    "        continue\n",
    "    texts.append(text)\n",
    "    labels.append(label)\n",
    "\n",
    "n = min(len(texts), 1000)\n",
    "if n == 0:\n",
    "    print('No labeled rows found to evaluate.')\n",
    "else:\n",
    "    texts = texts[:n]\n",
    "    labels = labels[:n]\n",
    "\n",
    "    # Try loading precomputed joblib embeddings (if available)\n",
    "    JOBLIB_PATH = os.path.join(os.getcwd(), 'pre-trained', 'novelty_embeddings.joblib')\n",
    "    emb_store = None\n",
    "    try:\n",
    "        data = joblib.load(JOBLIB_PATH)\n",
    "        emb_store = data.get('embs', None)\n",
    "        if emb_store is not None:\n",
    "            emb_store = np.asarray(emb_store)\n",
    "    except Exception as e:\n",
    "        emb_store = None\n",
    "\n",
    "    # Create embeddings for our evaluation texts (prefer SBERT, fallback TF-IDF)\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        eval_embs = embedder.encode(texts, convert_to_tensor=False, show_progress_bar=False)\n",
    "        eval_embs = np.vstack([np.array(e).astype(np.float32) for e in eval_embs])\n",
    "    except Exception as e:\n",
    "        print('SBERT not available; falling back to TF-IDF for evaluation:', e)\n",
    "        vec = TfidfVectorizer(max_features=1024, stop_words='english')\n",
    "        X = vec.fit_transform(texts).toarray()\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        norms[norms==0] = 1\n",
    "        eval_embs = (X / norms).astype(np.float32)\n",
    "\n",
    "    # Heuristic prediction: novelty = (1 - mean(top-k similarity to other docs)) * 100\n",
    "    preds = []\n",
    "    for i in range(len(eval_embs)):\n",
    "        past = np.delete(eval_embs, i, axis=0)\n",
    "        if past.shape[0] == 0:\n",
    "            preds.append(100.0)\n",
    "            continue\n",
    "        sims = cosine_similarity(eval_embs[i:i+1], past)[0]\n",
    "        k = min(5, len(sims))\n",
    "        topk = -np.sort(-sims)[:k]\n",
    "        mean_sim = float(np.mean(topk)) if len(topk) > 0 else 0.0\n",
    "        preds.append((1.0 - mean_sim) * 100.0)\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    mse = mean_squared_error(labels, preds)\n",
    "    rmse = mse ** 0.5\n",
    "    r2 = r2_score(labels, preds)\n",
    "    print(f'Count: {len(labels)}; MAE: {mae:.2f}; RMSE: {rmse:.2f}; R2: {r2:.3f}')\n",
    "\n",
    "    df = pd.DataFrame({'actual': labels, 'pred': preds})\n",
    "\n",
    "    # Scatter: actual vs predicted\n",
    "    plt.figure(figsize=(7,6))\n",
    "    sns.scatterplot(data=df, x='actual', y='pred', alpha=0.6)\n",
    "    plt.plot([0,100],[0,100], 'r--')\n",
    "    plt.xlabel('Actual Novelty (%)')\n",
    "    plt.ylabel('Predicted Novelty (%)')\n",
    "    plt.title('Actual vs Predicted Novelty')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Residuals histogram\n",
    "    df['error'] = df['pred'] - df['actual']\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.histplot(df['error'], bins=40, kde=True)\n",
    "    plt.title('Prediction Error Distribution (pred - actual)')\n",
    "    plt.xlabel('Error')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calibration: bin by actual novelty and show mean predicted vs actual per-bin\n",
    "    df['bin'] = pd.cut(df['actual'], bins=10)\n",
    "    calib = df.groupby('bin').agg(mean_actual=('actual','mean'), mean_pred=('pred','mean'), count=('actual','count')).reset_index()\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.lineplot(data=calib, x='mean_actual', y='mean_pred', marker='o')\n",
    "    plt.plot([0,100],[0,100],'r--')\n",
    "    plt.xlabel('Mean Actual Novelty (bin)')\n",
    "    plt.ylabel('Mean Predicted Novelty')\n",
    "    plt.title('Calibration by Actual Novelty Bins')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure joblib contains a fitted NearestNeighbors index (if missing) ---\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "JOBLIB_PATH = os.path.join(os.getcwd(), 'pre-trained', 'novelty_embeddings.joblib')\n",
    "try:\n",
    "    data = joblib.load(JOBLIB_PATH)\n",
    "except Exception as e:\n",
    "    data = {}\n",
    "if data:\n",
    "    texts = data.get('texts', [])\n",
    "    embs = data.get('embs', None)\n",
    "    try:\n",
    "        if embs is not None:\n",
    "            embs_arr = np.asarray(embs).astype(np.float32)\n",
    "            if data.get('nn') is None and embs_arr.shape[0] > 1:\n",
    "                try:\n",
    "                    from sklearn.neighbors import NearestNeighbors\n",
    "                    nn = NearestNeighbors(n_neighbors=min(10, embs_arr.shape[0]-1), metric='cosine', algorithm='auto')\n",
    "                    nn.fit(embs_arr)\n",
    "                    data['nn'] = nn\n",
    "                    joblib.dump(data, JOBLIB_PATH)\n",
    "                    print('Updated joblib with NearestNeighbors index')\n",
    "                except Exception as e:\n",
    "                    print('Failed to build or save NN index:', e)\n",
    "    except Exception as e:\n",
    "        print('Error while ensuring NN index:', e)\n",
    "else:\n",
    "    print('No pre-trained joblib found to update.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34060df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Demo: load the pre-trained joblib and score a sample text, then produce the formatted comment ---\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "JOBLIB_PATH = os.path.join(os.getcwd(), 'pre-trained', 'novelty_embeddings.joblib')\n",
    "try:\n",
    "    store = joblib.load(JOBLIB_PATH)\n",
    "except Exception as e:\n",
    "    print('Failed to load joblib:', e)\n",
    "    store = {}\n",
    "texts = store.get('texts', [])\n",
    "embs = store.get('embs', None)\n",
    "vec = store.get('vec', None)\n",
    "nn = store.get('nn', None)\n",
    "backend = store.get('backend', 'sbert' if embs is not None else 'tfidf')\n",
    "# load SBERT if available\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    sbert_local = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "except Exception:\n",
    "    sbert_local = None\n",
    "\n",
    "def embed_one(text):\n",
    "    if backend == 'sbert' and sbert_local is not None:\n",
    "        v = sbert_local.encode([text], convert_to_tensor=False, show_progress_bar=False)\n",
    "        return np.array(v[0]).astype(np.float32)\n",
    "    if backend == 'tfidf' and vec is not None:\n",
    "        X = vec.transform([text]).toarray().astype(np.float32)\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        norms[norms==0] = 1\n",
    "        return (X / norms)[0]\n",
    "    # fallback: simple TF-IDF on the fly\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        v = TfidfVectorizer(max_features=1024, stop_words='english')\n",
    "        v.fit([text] + texts)\n",
    "        X = v.transform([text]).toarray().astype(np.float32)\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        norms[norms==0] = 1\n",
    "        return (X / norms)[0]\n",
    "    except Exception as e:\n",
    "        print('Fallback embed failed:', e)\n",
    "        return np.zeros((1,), dtype=np.float32)\n",
    "\n",
    "def score_text(text, top_k=5):\n",
    "    emb = embed_one(text)\n",
    "    if embs is None:\n",
    "        return 100.0\n",
    "    emb_arr = np.asarray(embs).astype(np.float32)\n",
    "    if nn is not None:\n",
    "        try:\n",
    "            dists, idxs = nn.kneighbors([emb], n_neighbors=min(top_k, emb_arr.shape[0]))\n",
    "            sims = 1.0 - dists[0]\n",
    "            mean_sim = float(np.mean(sims)) if len(sims) > 0 else 0.0\n",
    "            novelty = (1.0 - mean_sim) * 100.0\n",
    "            return novelty\n",
    "        except Exception as e:\n",
    "            print('NN kneighbors failed:', e)\n",
    "    # fallback cosine similarity\n",
    "    try:\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        sims = cosine_similarity([emb], emb_arr)[0]\n",
    "        k = min(top_k, len(sims))\n",
    "        topk = -np.sort(-sims)[:k]\n",
    "        mean_sim = float(np.mean(topk)) if len(topk) > 0 else 0.0\n",
    "        novelty = (1.0 - mean_sim) * 100.0\n",
    "        return novelty\n",
    "    except Exception as e:\n",
    "        print('Cosine similarity fallback failed:', e)\n",
    "        return 100.0\n",
    "\n",
    "def format_comment(score):\n",
    "    score_int = int(round(score))\n",
    "    changeable = max(0, 100 - score_int)\n",
    "    # Short generic summary -- replace with Gemini-generated text in production if available\n",
    "    summary = \"The proposal introduces an integrated coal waste-to-energy process combining gasification and carbon-capture at pilot scale. The approach appears genuinely novel and should be prioritised for further validation.\"\n",
    "    recs = [\n",
    "        \"Document prior art and clearly highlight novel integration steps versus published work.\",\n",
    "        \"Provide pilot test plans and small-scale validation data to substantiate claimed innovations.\",\n",
    "        \"Include IP or patent landscape notes where applicable to strengthen novelty claims.\"\n",
    "    ]\n",
    "    # Build the formatted block exactly as requested\n",
    "    actions = ' '.join(recs)\n",
    "    comment = f\"Score: {score_int}/100 Changeable: {changeable}% {summary} Recommended actions: {actions}\"\n",
    "    return comment\n",
    "\n",
    "# Demo run: score the first stored text (if present) or a sample 500-word placeholder\n",
    "sample = texts[0] if texts else (\n",
    "    'This is a sample novelty text describing a pilot-scale integration of gasification with carbon capture aimed at coal waste. ' * 10\n",
    ")\n",
    "score = score_text(sample)\n",
    "print('Novelty score:', round(score, 2))\n",
    "print('\n",
    "\n",
    "print(format_comment(score))Formatted comment:\n",
    "')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
